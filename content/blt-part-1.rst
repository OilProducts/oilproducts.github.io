Stacked BLT's part 1
####################

:date: 2025-02-07 00:00:00
:category: Byte Latent Transformer
:summary: Reproducing the Byte Latent Transformer paper at home.  Part one of a series where I try to reproduce and improve upon (lol) the `BLT paper <https://arxiv.org/abs/2412.09871>`_

This is the first part of a series of posts where I attempt to reproduce a recent paper.  I've not written long form content in a while, so I'm going to start with a more stream-of-consciousness style and not really filter anything between thought and keyboard.  I hope to get better at this as I go along.  Perhaps at the end I'll edit it into something more readable.

In this post, Iâ€™ll give an overview of my motivations, outline why I believe current LLMs are not sufficient for human-like AI, and then introduce the BLT paper and my initial thoughts on reproducing it.

I've had a lot of nebulous thoughts about how a hypothetical AI might work.  One of those thoughts that I've tried a few times to get in to words is near to the idea that LLMs, while intelligent, are probably not sufficient to create a human like AI system.  There are a few reasons why I believe this to be the case, the first is that LLMs by the nature of their existence experience the world very differently from humans.  Their perception is episodic, and limited to what can be expressed in tokens (whether that's a significant limitation is debatable though).  I don't presume to understand the many ways that humans experience their internal selves but I do know that I often have thoughts or concepts that are "close" to a word that I know but with some nuance that isn't quite captured or easily expressed via words.  And while this "close to a word" could be thought of as something akin to the latent representation of an LLM prior to the last pass through the embedding matrix for a token with high uncertainty, it is (in my opinion) a bit of a stretch to make that claim outright.  I think there would be a much stronger claim to be made if the "token" could somehow represent something closer to what I consider a unit of thought.  I am very aware that my opinions or intuition here does not necessarily represent anything real, but that's how I start my tinker projects.

For a long time I had no useful ideas for how this problem might be approached, until I read `Byte Latent Transformer: Patches Scale Better Than Tokens <https://arxiv.org/abs/2412.09871>`_.  The paper proposes a method of dynamic tokenization that sort of, well "equalizes information content" isn't right, but the tokenization is based on the uncertainty of the next token of a "small Large Language Model" trained on bytes as the tokens. A byte is considered to be a token boundary if the entropy of the models prediction has increased from the previous token.  The result is a context dependent tokenization system that could turn the same phrase into different sets of tokens based on the longer context in which it appears.  If one completion would be less predictable than another, the less predictable version would have more token boundaries and therefore have more tokens to attend to.  There is a lot more plumbing that makes this work.  Once the tokens have been determined theres a whole scheme to encode them into "patches" which are then fed into a typical (as of early 2025) decoder only transformer and then on the back end to decode the patches back in to text.

This is all very clever.

My first thought after I understood the paradigm was "we have to go deeper".  The examples in the paper are mostly typical tokenish sized patches, representing on average four to five characters.  But hey, since we're already training a model on bytes to make patches which are encoded to feed into a regular transformer, why cant we then use that inner transformer to feed another layer, where the uncertainty of prediction for the next patch is used to determine a higher order patch boundary.  If we end up with 4-5 first order patches per second order patch we now have a unit that is more like a phrase.  One more level and I might even call it an idea, which is exactly the kind of unit I want to work with.

So that is my goal here.  First, recreate the BLT paper, but probably scaled down, as I am but a humble hacker with only a single GPU.  Then, once I have a working model, I will try to add a second layer of patching to see if I can get a more human like unit of thought.  Along the way I will try to document the process and write readable code that others can tinker with.